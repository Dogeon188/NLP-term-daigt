{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>LLM - Detect AI Generated Text</center>\n",
    "\n",
    "This competition challenges participants to develop a machine learning model that can accurately detect **whether an essay was written by a student or an LLM**. The competition dataset comprises a mix of student-written essays and essays generated by a variety of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Members: 毛柏毅, 朱誼學, 許木羽, 張立誠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as T\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOST: Localhost, IS_RERUN: None\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "HOST: Literal['Localhost', 'Interactive', 'Batch'] = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost')\n",
    "IS_RERUN: bool = os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "\n",
    "print(f'HOST: {HOST}, IS_RERUN: {IS_RERUN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    (\"cuda:0\" if torch.cuda.is_available()\n",
    "     else \"mps\" if torch.backends.mps.is_available()\n",
    "     else \"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaggle_csv(dataset: str, name: str, is_comp: bool = False) -> pd.DataFrame:\n",
    "    assert name.endswith('.csv')\n",
    "    if IS_RERUN:\n",
    "        return pd.read_csv(f'/kaggle/input/{dataset}/{name}')\n",
    "    if is_comp:\n",
    "        path = kagglehub.competition_download(dataset)\n",
    "    else:\n",
    "        path = kagglehub.dataset_download(dataset)\n",
    "    return pd.read_csv(Path(path) / name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "if IS_RERUN:\n",
    "    df_train = get_kaggle_csv('daigt-datamix', 'train_essays.csv')\n",
    "    df_test = get_kaggle_csv('llm-detect-ai-generated-text', 'test_essays.csv', is_comp=True)\n",
    "else:\n",
    "    df = get_kaggle_csv('dogeon188/daigt-datamix', 'train_essays.csv')\n",
    "    # split df_train into train and test\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df_test = df.iloc[-1000:].copy()\n",
    "    df_train = df.iloc[:5000].copy()\n",
    "    # Up sampling -> used to balance the number of data (generated = 0 or 1)\n",
    "    human = df[df['source']=='human'].copy().sample(frac=1).reset_index(drop=True)\n",
    "    minority = df_train[df_train['source']=='human'].shape[0]\n",
    "    majority = df_train[df_train['source']!='human'].shape[0]\n",
    "    up_sampling = human[:majority-minority]\n",
    "    df_train = pd.concat((df_train, up_sampling)).sample(frac=1).reset_index(drop=True)\n",
    "    assert df_train[df_train['source']=='human'].shape[0] == df_train[df_train['source']!='human'].shape[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T.AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=False)\n",
    "tokenizer.save_pretrained(\"./src/tokenizer\")\n",
    "\n",
    "# tokenizer = T.AutoTokenizer.from_pretrained(\"src/tokenizer\")\n",
    "\n",
    "# Define the hyperparameters\n",
    "lr = 3e-5\n",
    "epochs = 3\n",
    "train_batch_size = 16\n",
    "validation_batch_size = 16\n",
    "test_batch_size = 16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0       3\n",
      "1       8\n",
      "2       5\n",
      "3       5\n",
      "4       7\n",
      "       ..\n",
      "8731    5\n",
      "8732    6\n",
      "8733    8\n",
      "8734    5\n",
      "8735    7\n",
      "Name: source, Length: 8736, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Encoded\n",
    "source_classes = {'claude': 0, 'cohere': 1, 'falcon': 2, 'gpt': 3, 'llama': 4, 'mistral': 5, 'palm': 6, 'T5': 7, 'human': 8}\n",
    "df_train.drop(df_train[df_train['source'] == 'unknown'].index, inplace=True)\n",
    "df_test.drop(df_test[df_test['source'] == 'unknown'].index, inplace=True)\n",
    "\n",
    "if df_train['source'].dtype != \"int64\":\n",
    "    df_train['source'] = df_train['source'].apply(lambda x: source_classes[x])\n",
    "    df_test['source'] = df_test['source'].apply(lambda x: source_classes[x])\n",
    "print(df_train['source'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\", \"test\"]\n",
    "        if split != 'test':\n",
    "            self.data = df[split]\n",
    "        else:\n",
    "            self.data = df\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data.iloc[index]\n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    source = [item['source'] for item in batch]\n",
    "\n",
    "    encoded_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    source_tensor = torch.tensor(source)\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoded_inputs['input_ids'],\n",
    "        'token_type_ids': encoded_inputs['token_type_ids'],\n",
    "        'attention_mask': encoded_inputs['attention_mask'],\n",
    "        'source': source_tensor\n",
    "    }\n",
    "\n",
    "split_ratio = 0.85\n",
    "split_idx = int(len(df_train) * split_ratio)\n",
    "df_split = {\"train\": df_train[:split_idx], \"validation\": df_train[split_idx:]}\n",
    "\n",
    "\n",
    "ds_train = CustomDataset(df_split, \"train\")\n",
    "ds_validation = CustomDataset(df_split, \"validation\")\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=train_batch_size, collate_fn=collate_fn)\n",
    "dl_validation = torch.utils.data.DataLoader(ds_validation, batch_size=validation_batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    predictions = logits.argmax(axis=-1).cpu().numpy()\n",
    "    labels = labels.cpu().numpy()\n",
    "    acc = accuracy_score(labels, predictions)\n",
    "    return acc\n",
    "\n",
    "class Deberta(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.deberta = T.AutoModel.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=9)\n",
    "        self.deberta.save_pretrained(\"./src/deberta\")\n",
    "        self.deberta.gradient_checkpointing_enable()\n",
    "\n",
    "        self.linear = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 384),\n",
    "            torch.nn.Linear(384, 9)\n",
    "        )\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "    def forward(self, **kwargs):\n",
    "        input_ids = kwargs['input_ids']\n",
    "        attention_mask = kwargs['attention_mask']\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Mean pooling\n",
    "        # reshape attention_mask, used to filter the valid tokens\n",
    "        mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()  # [batch_size, seq_len, hidden_size]\n",
    "        # summation for valid tokens\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask, dim=1)  # [batch_size, hidden_size]\n",
    "        # num of valid tokens\n",
    "        sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "        x = sum_embeddings / sum_mask\n",
    "        \n",
    "        x = self.linear(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return {\n",
    "            'source': x\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deberta().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=lr)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "def save_checkpoint(epoch, model, optimizer, path=\"./checkpoints\"):\n",
    "    os.makedirs(path, exist_ok=True)\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "    }, os.path.join(path, f\"checkpoint_epoch_{epoch}.pt\"))\n",
    "\n",
    "def load_checkpoint(path, model, optimizer):\n",
    "    checkpoint = torch.load(path)\n",
    "    model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    start_epoch = checkpoint['epoch']\n",
    "    return model, optimizer, start_epoch\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/3]: 100%|██████████| 465/465 [08:03<00:00,  1.04s/it]\n",
      "Validation epoch [1/3]: 100%|██████████| 82/82 [00:28<00:00,  2.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5597560975609756\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [2/3]: 100%|██████████| 465/465 [09:01<00:00,  1.17s/it]\n",
      "Validation epoch [2/3]: 100%|██████████| 82/82 [00:24<00:00,  3.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5520833333333334\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [3/3]: 100%|██████████| 465/465 [08:57<00:00,  1.16s/it]\n",
      "Validation epoch [3/3]: 100%|██████████| 82/82 [00:24<00:00,  3.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.5765752032520326\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        source = batch['source'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        loss = loss_fn(pred['source'], source)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        acc, cnt = 0, 0\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            source = batch['source'].long().to(device)\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Scoring\n",
    "            acc += compute_metrics((pred['source'], source))\n",
    "            cnt += 1\n",
    "        \n",
    "        print(f\"accuracy: {acc/cnt}\")\n",
    "    save_checkpoint(ep, model, optimizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./src/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 63/63 [00:21<00:00,  2.89it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy: 0.3134920634920635\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# model = torch.load(\"./src/model\")\n",
    "model = Deberta().to(device)\n",
    "model.load_state_dict(torch.load('./src/model', weights_only=True))\n",
    "\n",
    "ds_test = CustomDataset(df_test.copy(), \"test\")\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=test_batch_size, collate_fn=collate_fn)\n",
    "\n",
    "pbar = tqdm(dl_test)\n",
    "pbar.set_description(f\"Test\")\n",
    "model.eval()\n",
    "\n",
    "generated_preds = []\n",
    "source_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "        acc, cnt = 0, 0\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            source = batch['source'].long().to(device)\n",
    "            \n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            tmp = torch.argmax(pred['source'], dim=-1).cpu().tolist()\n",
    "\n",
    "            source_preds.extend(tmp)\n",
    "            generated_preds.extend(list(map(lambda x: (sum(x[:8].cpu().tolist())) / (sum(x[:8].cpu().tolist()) + x[8].cpu().item()), pred['source'])))\n",
    "\n",
    "            # Scoring\n",
    "            acc += compute_metrics((pred['source'], source))\n",
    "            cnt += 1\n",
    "        \n",
    "        print(f\"accuracy: {acc/cnt}\")\n",
    "\n",
    "# final_preds = ...  # should be a 1D array of predictions, with the same length as df_test, and values in [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5002552871719624, 0.9999687785492802, 0.9999356819238724, 0.5002404161722352, 0.5002024046643971, 0.532119064025266, 0.5002529569437665, 0.5006680054588466, 0.5002162544955341, 0.5001974236077423, 0.5003945570672582, 0.9998087658619547, 0.5024606952787399, 0.5002489422754923, 0.9997855663381677, 0.987430032852393, 0.9999388667296992, 0.9997967689079676, 0.5002429690187592, 0.5002364125836518, 0.5002001553755355, 0.9998656701856108, 0.9994926233565962, 0.9998964186078848, 0.5002617891567097, 0.9997507453871575, 0.9998082438545349, 0.9998860289122952, 0.6229459496040772, 0.5002773938800464, 0.5002834149780241, 0.9998731795855911, 0.500187308058815, 0.9947859040440964, 0.9998440989877337, 0.500268992453873, 0.5002537239771867, 0.5003350454930622, 0.500255309810736, 0.9999238464898779, 0.9851676209444827, 0.9817326159645572, 0.9998439893038725, 0.5002691977194956, 0.9994632241264262, 0.5002076283484678, 0.5002413947023836, 0.5002762100896461, 0.9869074522901395, 0.9999027946557588, 0.9999060617648121, 0.500288117385468, 0.9999289045650722, 0.5002391832490439, 0.9997961238406001, 0.9998756342001173, 0.5002139367077844, 0.9960457474299176, 0.5002056365148035, 0.9998322042714142, 0.5005961665002235, 0.5001877514531003, 0.9999403687471263, 0.5002294773845525, 0.9998886405839198, 0.9988197298567011, 0.5002061602269317, 0.9887251735563275, 0.5001896890558978, 0.5218722853663814, 0.5023306916830275, 0.5003018222562383, 0.9999057099661762, 0.5002944476731028, 0.9997511450936787, 0.9997924728211356, 0.9999251711759495, 0.9995018561117271, 0.9999124116941531, 0.9995660225624882, 0.500223145130088, 0.9997962156065093, 0.5002225823286417, 0.9997554691103613, 0.9992897630644244, 0.5002353863926526, 0.5001626137883287, 0.9998703511506977, 0.5002108910978769, 0.7980271237364431, 0.9998123078224123, 0.5002392666567331, 0.500237715676842, 0.5659910546731953, 0.5002536175375617, 0.9996663984825501, 0.5446586887939695, 0.9999506401155023, 0.5002595594646367, 0.9992681074306864, 0.500208753401967, 0.9998849470965376, 0.9996903089601487, 0.9999079972189276, 0.5125683768361101, 0.5002455831873656, 0.9999342129293866, 0.5002027258370332, 0.9997707572797446, 0.9998843012517674, 0.5002343199279173, 0.5002285220171526, 0.5002500890334511, 0.99978684348167, 0.9937971805136313, 0.999822630781771, 0.521442134813786, 0.500207898526288, 0.9997516943817931, 0.5002237781846032, 0.9997362132051268, 0.9997528694768314, 0.5490700047921242, 0.5002076562480706, 0.9998931034377427, 0.999833401159949, 0.9998997995265717, 0.9997991128773811, 0.9998733608399405, 0.500187553986259, 0.9997156396720387, 0.50028917724288, 0.9997865682408926, 0.9976929796816268, 0.5002228292227823, 0.5002194967396111, 0.9997609681553071, 0.9999490132680101, 0.5011114318924621, 0.9998464290313005, 0.5002283709548326, 0.5002278175508603, 0.5002514124862165, 0.9818558603631967, 0.5002321095727243, 0.5002419750203293, 0.9998780037601666, 0.9998881428457131, 0.9995323689889147, 0.5002684813090192, 0.5006093984909219, 0.5002350187405676, 0.5003557623622431, 0.9999158743545614, 0.9999058604126528, 0.9998719186781645, 0.9999082290041764, 0.500163989417206, 0.9999373613780412, 0.5002125257125395, 0.5012838273792137, 0.9900732158702853, 0.9998958014295634, 0.5002397502806782, 0.5002407656222928, 0.5002784961877045, 0.5049455392497904, 0.999898156521786, 0.5002278449592186, 0.5002498624593026, 0.9999545017825555, 0.9997806116888379, 0.5002406467422901, 0.9998993783644489, 0.9998351659330742, 0.9881292066439776, 0.9999492662869564, 0.9833857660707355, 0.9998943290138406, 0.9999667833544191, 0.5002436698233789, 0.5002323599050704, 0.5002539594879551, 0.500250454231948, 0.9999250633159391, 0.5002442042838325, 0.5001798804463808, 0.9999047799725436, 0.9999384782510362, 0.9998727387139626, 0.5002413068956179, 0.9840567070952773, 0.5000819166917697, 0.5002281175359595, 0.5002280319282133, 0.5002145542688583, 0.999902556315044, 0.9997488707775783, 0.5002260777583523, 0.9999122180239618, 0.5001892979389648, 0.500286339933054, 0.5002376854620446, 0.9414544945029653, 0.5002452438247672, 0.500245521287885, 0.5007220108336533, 0.9982482442771379, 0.5003620111785891, 0.999938182493351, 0.9999089816298571, 0.9995292640023378, 0.5002374628919216, 0.9999155709463434, 0.9999000392005453, 0.99991366278438, 0.5002906959073591, 0.5047921355497235, 0.9998152342727378, 0.9998899678646861, 0.5002191977859802, 0.5002376376169098, 0.5006753624157914, 0.999869503313222, 0.5002511077676239, 0.9998274557533994, 0.500191741933191, 0.5002502372710611, 0.5002839151025154, 0.5037957750549586, 0.9997774833525644, 0.5002046540283474, 0.5002383196794268, 0.5003114455751253, 0.5002736926765126, 0.4996947769445764, 0.5002531310941732, 0.5002019857795715, 0.5002533946757631, 0.5002823362043953, 0.500311809324156, 0.9625981825092299, 0.9998806624608887, 0.9999148502289034, 0.9999533136197944, 0.999641929388498, 0.9997969936219213, 0.500199465595697, 0.5001867036510473, 0.5001573623613393, 0.500291782180591, 0.500227319158245, 0.5003243266959834, 0.9998070677699477, 0.9366352159219937, 0.6683234523488413, 0.5002259621498456, 0.5002325288047371, 0.985344981676647, 0.5002438648242689, 0.9997444483620843, 0.5002467180220738, 0.9999321302391526, 0.5002037522217453, 0.9998275817406385, 0.9999470717151524, 0.9998201901364543, 0.6419856136277724, 0.5002513944934739, 0.9999015964975183, 0.5002428764162651, 0.9789728498537638, 0.9999687781984048, 0.5002302666754143, 0.5001766125227549, 0.5002359847645034, 0.9963875903868855, 0.5002576476844994, 0.5003544767361924, 0.9997868331456401, 0.9998692183779633, 0.5695178010207683, 0.9998616829018624, 0.9569681192491233, 0.9998985537563881, 0.9997891079597023, 0.9997322692494872, 0.9998184297112795, 0.5002627531827789, 0.9999636949494101, 0.8437848520220881, 0.9997695967016761, 0.9990405294142919, 0.500205852527982, 0.9999247508023211, 0.9998117326919294, 0.9999610558730648, 0.9999240986058858, 0.5001918787636506, 0.9999672853041243, 0.5002016047820533, 0.5004295892071592, 0.5003248631379911, 0.5002022548992775, 0.9999167038217485, 0.4990621855268605, 0.500225539733784, 0.5002011600911958, 0.500227745392596, 0.5482479923608758, 0.5002516753276105, 0.5002297588266679, 0.5001963722783027, 0.5002346217792267, 0.5000776277221884, 0.5002818097865347, 0.5004853579183445, 0.9999137910550081, 0.5002653758358506, 0.5002147603272136, 0.9998990020451668, 0.5014601427077298, 0.5002410955038057, 0.9999421353961135, 0.9979355293389722, 0.5002191245425636, 0.5002297522886061, 0.9977014018404425, 0.999789829366944, 0.5002260557981778, 0.999498426444124, 0.9313718875422509, 0.9997787992467693, 0.500247655365547, 0.5001804761872782, 0.5002214325401447, 0.9998400187586514, 0.972461559607293, 0.5012546776010824, 0.9998525981508656, 0.5003006807578562, 0.500818556268622, 0.5002301598741352, 0.49973460928019553, 0.9996882344018896, 0.9999236344467206, 0.9998669420130327, 0.5002030348662959, 0.5001817014771828, 0.500253321033544, 0.9999536114102991, 0.9998969540320949, 0.5002700566955074, 0.5002414448369868, 0.9999361648904436, 0.9999449876159475, 0.9999188056320696, 0.9941518172790389, 0.5002920692577084, 0.5002811774292346, 0.9998808390584047, 0.9990856566772314, 0.5126825502452106, 0.9990878710805755, 0.500234942452408, 0.5007922814346459, 0.9973731307986531, 0.5002401938724672, 0.9996996117848528, 0.5002890081524833, 0.5004911770655233, 0.5002275360444145, 0.9998519895565832, 0.5002349126260639, 0.9998000495586454, 0.5006874419342837, 0.5008681901419387, 0.5002456342720056, 0.5002462121090152, 0.9964385557058507, 0.5002077204872852, 0.9998421438356091, 0.9999375082225241, 0.5002473339529786, 0.9997728176515379, 0.9989300200989956, 0.5002300513258369, 0.5002082824095225, 0.5002237450804143, 0.5003517828599997, 0.999813117546382, 0.5002371503433474, 0.5011305027364193, 0.5002019723685682, 0.5002104132225098, 0.9999233150489573, 0.9990112161004033, 0.9998657012209035, 0.5002288576589549, 0.9954723320177549, 0.5001133951640941, 0.5002501932215857, 0.5002548508493263, 0.9996523706436254, 0.5003002521680725, 0.9967410981131557, 0.5002517958313112, 0.9985089825481276, 0.5001447590275327, 0.9998619733532357, 0.5002407890111026, 0.9990740511134424, 0.5001981429765205, 0.5002120685137172, 0.5002354964942246, 0.5002359125939834, 0.5003900388621367, 0.5002442195607734, 0.5512711487571864, 0.5002155353557873, 0.500222002147388, 0.5003962836196999, 0.9998311299679713, 0.9998775229727092, 0.9916341284853725, 0.5002446651566663, 0.9996500810266776, 0.5002363208105823, 0.9997740871655744, 0.5002314563582053, 0.5525061960065957, 0.5002148725607928, 0.5002441251658307, 0.999839943077249, 0.9997985180056725, 0.9999475629310486, 0.9997930587982313, 0.5002307737155246, 0.5002554242004655, 0.5002244568557831, 0.9994074203296052, 0.500242940893272, 0.9997649887822158, 0.5002303415566266, 0.500218457962239, 0.9997771340738839, 0.9991925436727278, 0.9998069719776734, 0.9999702052501973, 0.9972504552262715, 0.5918498557489841, 0.5002020179909322, 0.9998570393856386, 0.5002946903650592, 0.5002260947067914, 0.5002706374626751, 0.5002753335119096, 0.9998148624299584, 0.999823399562292, 0.9998858865294761, 0.5002367552654716, 0.5002311729765432, 0.5002284982047317, 0.9995763206004704, 0.5236137547225203, 0.500222746004856, 0.9996116491997913, 0.5002455980700573, 0.5006387034596214, 0.790916674857102, 0.5002543755217321, 0.5002304100209042, 0.999822722433191, 0.5005300773290692, 0.9998448483985188, 0.5005799956333005, 0.5002361487417281, 0.5002289546653435, 0.500287429791522, 0.9997938503048341, 0.9999580918075963, 0.5002405418103768, 0.500207515399678, 0.9999358045630021, 0.9962708622099928, 0.9999317796013083, 0.5022553529964133, 0.9998604053313883, 0.5002276781759126, 0.9998079290952743, 0.9998167841719368, 0.5002121655102467, 0.99991186994453, 0.9998194825377236, 0.5002235966099912, 0.9997596319281621, 0.9999443031791961, 0.9989640102014959, 0.5002262045822226, 0.5001862272481262, 0.5003127065215898, 0.9999502182102606, 0.9997221494970725, 0.5002184725629335, 0.5003409028503033, 0.9784214318123843, 0.5002306657772051, 0.5002509187311448, 0.5002340801637983, 0.5941946175182353, 0.5231653267903715, 0.5002311006683632, 0.5002459457873725, 0.9998058593569222, 0.5002674003304057, 0.5002366884640889, 0.5000618389599644, 0.5002422750633977, 0.9971746033572597, 0.999958389957028, 0.500252120906008, 0.9999530545680586, 0.9998239107180875, 0.9998668151301329, 0.9999133979138156, 0.9995357882438681, 0.999924398942753, 0.5002063407003646, 0.4936928937848149, 0.627336317191189, 0.5002834855169577, 0.5002540466343797, 0.5004218389458857, 0.999867864838987, 0.9837298773267117, 0.5019742885857944, 0.5001977369892432, 0.5001466075806466, 0.5011252627515156, 0.5002363609372823, 0.9997262165525401, 0.9999450377588991, 0.9998351993755193, 0.5000698266842993, 0.5022760570365082, 0.9998536176782247, 0.5013524936902796, 0.5000602140981194, 0.500257139490489, 0.5002492268235448, 0.9830548438812604, 0.5002266023638756, 0.5002626568588637, 0.50021042655556, 0.9999219553372859, 0.5005577424809867, 0.5002687123434634, 0.9999390002686463, 0.5002162107512775, 0.5002383995523417, 0.5002121751662127, 0.9998463897878543, 0.999894106731, 0.5002432855293528, 0.999841841020898, 0.9998074744129495, 0.9999548767110521, 0.5002164338407487, 0.5001799732959759, 0.5003237156425147, 0.9998031565700108, 0.9999074991888479, 0.5002270411358052, 0.9686957305340829, 0.9997962325543754, 0.9998299178657657, 0.6421669240138344, 0.999719832504606, 0.5001949552375756, 0.5002254919304416, 0.9998061122650719, 0.50023838145824, 0.99982378862971, 0.5002377928311713, 0.5002465853294731, 0.8365445017171392, 0.5002263006752197, 0.9999132576742056, 0.5002622921169406, 0.5002456820306986, 0.9999494648881216, 0.5002109381401184, 0.5001483885714194, 0.9998559562290602, 0.5002368410619611, 0.5002435633968563, 0.9999149784458423, 0.9998960030129541, 0.5002589659103603, 0.9998826939279373, 0.9997777442303507, 0.5002564132643036, 0.5002320190260459, 0.501036241428424, 0.5002081606895205, 0.5022675436257801, 0.500196866987558, 0.9998040457623958, 0.5019570856515818, 0.5835133142427162, 0.5002109097412614, 0.500770476283785, 0.5002257928053034, 0.5002290041596383, 0.5002257337707695, 0.9979951203151546, 0.9996723682603378, 0.5002160699688153, 0.9955937375562399, 0.5883953953106551, 0.5003146372096625, 0.5002271741646417, 0.5003161395619902, 0.9999516925999631, 0.500230970621019, 0.5002030778313982, 0.5002501716467928, 0.9998024260280919, 0.5002304502849909, 0.9996393101046785, 0.5004750311174124, 0.5002318055935437, 0.5670166191888133, 0.5001907162043493, 0.5002747686439544, 0.4807895611792846, 0.999767391943064, 0.5002211887295447, 0.5002610154305028, 0.9999001188279086, 0.5002303715086074, 0.5192959066566276, 0.5002773502061292, 0.9998725753710185, 0.9978385188525831, 0.5002106977288994, 0.5002561034379576, 0.999720109165657, 0.5002092105756234, 0.5008857842220045, 0.9999756358482588, 0.9999157451941649, 0.999926180621352, 0.5002282709511636, 0.5045263925855443, 0.5001149506942887, 0.5002294175122193, 0.5003224154307752, 0.5002352836604287, 0.5008969636733817, 0.5002376471092969, 0.9997544836433097, 0.5002365938095168, 0.9999450151882495, 0.5002256165829054, 0.5001817660542965, 0.5002665556811235, 0.9999625303760362, 0.5002330860412024, 0.9998218379686103, 0.5007377213767972, 0.9998655897060793, 0.9999495213277473, 0.9999399169409857, 0.5002444259705927, 0.9998539202024346, 0.5002825623473263, 0.5002339583366905, 0.5002120700243967, 0.5002234704847671, 0.9997809927950686, 0.9999442936196603, 0.5002579142967892, 0.9998762092375576, 0.9936070403251792, 0.5002371829732998, 0.9999085984828893, 0.9998186565313973, 0.6319565846311009, 0.9999083483953156, 0.5002846389733974, 0.5001990164231194, 0.5009591828002955, 0.999945430329179, 0.5002282437462889, 0.5002174566724406, 0.9856940134698684, 0.5002168136567833, 0.9997795026723132, 0.9994385276735032, 0.9996646300371749, 0.9997886992529296, 0.9909347304144336, 0.9998688880825787, 0.9998188227388358, 0.9998567578340947, 0.9998835806076819, 0.9998842656370848, 0.9999220231772097, 0.9998649962446654, 0.5002237892875016, 0.5480141069603739, 0.5005521262432427, 0.9998450917849115, 0.9999191088001824, 0.5002334963499703, 0.6148254489602804, 0.9999030763939473, 0.98429501211319, 0.5002128153635285, 0.5518340609681673, 0.9998215713746454, 0.9999038475515337, 0.5004630897874377, 0.9998118398789853, 0.9997747533182156, 0.5001836633392305, 0.9999548723298857, 0.9980674102396324, 0.9999501961066599, 0.5002068229527391, 0.9999317629700215, 0.9998450456038777, 0.5008375240473925, 0.5002488141626983, 0.9485957368082195, 0.5001999805148164, 0.5002126106025332, 0.5002836078550241, 0.9998411690364639, 0.9980028435837209, 0.5308846680851085, 0.9998209658618386, 0.9997388010002682, 0.5002821435884256, 0.5002334549165666, 0.9971220072619191, 0.5002604540624331, 0.5003879072706313, 0.5003771998817138, 0.9999110399857224, 0.5002403563328239, 0.9998374232273096, 0.9998359007592543, 0.9999031236250475, 0.5125145753880487, 0.5002007939107965, 0.5001961412081972, 0.9756596083878525, 0.500243295662781, 0.9999110139579076, 0.5001860615783418, 0.5002440326249916, 0.500324361039518, 0.550427378364371, 0.5013405693245028, 0.9998710758701531, 0.5002447004308471, 0.5002291429475286, 0.999858328382242, 0.5002247498899686, 0.9996209313332316, 0.5002037176517382, 0.5006598326644858, 0.9991690559623143, 0.9971770355143936, 0.5032786275500774, 0.9999389164938157, 0.5002434573247193, 0.5002358455669534, 0.9999436912925246, 0.5002332454277458, 0.5002944160411689, 0.5001996852948154, 0.5002216359916642, 0.999866938149104, 0.9996998546430311, 0.5002236200964231, 0.5002192869461785, 0.9840416966701032, 0.5002344365261621, 0.9999276080035786, 0.5002324825071425, 0.5002523338410192, 0.5002780362267087, 0.9999288899810153, 0.500218755535253, 0.98023351331097, 0.999801684660864, 0.9998102602641912, 0.9838284108384169, 0.9998623022957845, 0.9999310291731726, 0.5002243201395878, 0.9934301794897199, 0.500232176381989, 0.5002763663948046, 0.9997468427507177, 0.5002196823819856, 0.9999070046907278, 0.5002292675462375, 0.9998640471297975, 0.5002488185940287, 0.7045283178525011, 0.5002053739288637, 0.5002643912132487, 0.5002513761997508, 0.9996942507588967, 0.9998881695136942, 0.5003080128357643, 0.9998481716473893, 0.5002410223200029, 0.5002095752728962, 0.9974384977658144, 0.9989366668198235, 0.5002239415768305, 0.6440221445387588, 0.5002318822270666, 0.5002330761222248, 0.9803115490623912, 0.9992449483421083, 0.9999415618151115, 0.500346056029922, 0.9998587978340868, 0.5002215814364562, 0.5003191105473717, 0.9999476873139033, 0.5003293711697349, 0.5012808889263013, 0.5127599198692935, 0.9971710841333069, 0.6191180207029766, 0.5002455678484247, 0.5027698481517229, 0.9996113166638972, 0.9998598013335622, 0.5002337879446955, 0.5004880466533391, 0.5002171496366346, 0.5025009856527651, 0.4994543817745819, 0.500219131881278, 0.5002880071224163, 0.9997676713619715, 0.9998444906507716, 0.9999135481621229, 0.8840985339925942, 0.5002422071505348, 0.9991479023418437, 0.5002637310406988, 0.9997602259250121, 0.5002093883605672, 0.500420268567541, 0.5001735429673563, 0.9998142758992261, 0.5002413893538049, 0.9991173210719881, 0.5475006722233813, 0.5002360885266517, 0.999756325165771, 0.5102733274195355, 0.9998161760902837, 0.5128729812059397, 0.5005321721991959, 0.9999477529937879, 0.5002181374185756, 0.9985497372320746, 0.5002710717389784, 0.5007137164558838, 0.9999701463373162, 0.9998342624877912, 0.9992184035936803, 0.982391868218548, 0.990486732138276, 0.9112252438231802, 0.5000293592314508, 0.5012262304591314, 0.9995860789998794, 0.9989615503891631, 0.9998459943128426, 0.9999574000733896, 0.999766263100094, 0.5002221319144188, 0.5003232796907187, 0.9997071934637746, 0.9998437539751768, 0.999971056389731, 0.5002197330300596, 0.5001875466087409, 0.5002485385111238, 0.5002272428462944, 0.5332226296249583, 0.5022854729649359, 0.5002151375423306, 0.5002337612472498, 0.5002521970331257, 0.500240667027413, 0.5002416288312156, 0.9997798498274375, 0.5002192937328139, 0.9997127975847991, 0.9964348010453851, 0.5002554883181776, 0.9997926304812343, 0.5002408058717688, 0.5002507176486303, 0.9998756800854076, 0.5002418769870294, 0.9996578677197948, 0.5007655502503752, 0.9998978608019177, 0.9934169368855144, 0.5002449406900913, 0.9998528302986058, 0.9998572756245541, 0.5002621682816237, 0.5005260086349645, 0.9998628884246482, 0.9999284685467152, 0.5001667657774709, 0.9999419761523464, 0.9999168479086994, 0.9997848808779158, 0.9998655740066101, 0.9998684862760353, 0.5002764523506067, 0.9999267680746525, 0.5010371422726406, 0.9998174768847531, 0.5002484002726719, 0.9997916420913954, 0.9998043612417613, 0.5002581347017466, 0.5010797698372973, 0.5003341984866059, 0.9999273223705599, 0.5002645320557513, 0.999880432678491, 0.9999278986536705, 0.5002195655558773, 0.5002172825924555, 0.9999083295744369, 0.9997497834019241, 0.5018574860616394, 0.9998895591265221, 0.9999301066240542, 0.5002110785659869, 0.999914771126469, 0.5002274285884033, 0.9997674720770301, 0.9998735079891393, 0.975392748243267, 0.9995388632509806, 0.9998624844080014, 0.9999218015675958, 0.7638948917577351, 0.9999713648651327, 0.500256792577754, 0.5002397258965259, 0.5002477261283541, 0.5003343522627407, 0.9999149931985029, 0.5002546257613724, 0.5001851452354367, 0.5002750389907132, 0.5002037655376688, 0.9998805610472705, 0.734314349704282, 0.500198309785134, 0.999867428703471, 0.9999300700340572, 0.9998824674622725, 0.5002342644447095, 0.9998441983390653, 0.5001642840287956, 0.9999435506991637, 0.5061281017368904, 0.9998247962035799, 0.5002235227094396, 0.5002413419807474, 0.9998360553482866, 0.9998550033239608, 0.9985068797306932, 0.9998023896011882, 0.5085942170309666, 0.9993294816075067, 0.9998096876651198, 0.9998529689038786, 0.5002523874980699, 0.9998606497996868]\n",
      "98836    1\n",
      "98837    1\n",
      "98838    1\n",
      "98839    1\n",
      "98840    0\n",
      "        ..\n",
      "99831    1\n",
      "99832    1\n",
      "99833    1\n",
      "99834    1\n",
      "99835    1\n",
      "Name: generated, Length: 1000, dtype: int64\n",
      "ROC AUC: 0.8057\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "if not IS_RERUN:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    print(generated_preds)\n",
    "    print(df_test['generated'])\n",
    "    auc_score = roc_auc_score(df_test['generated'], generated_preds)\n",
    "    \n",
    "    print(f\"ROC AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['generated'] = generated_preds\n",
    "submission = df_test[['id' if IS_RERUN else 'prompt_id', 'generated']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
