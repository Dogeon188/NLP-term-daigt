{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>LLM - Detect AI Generated Text</center>\n",
    "\n",
    "This competition challenges participants to develop a machine learning model that can accurately detect **whether an essay was written by a student or an LLM**. The competition dataset comprises a mix of student-written essays and essays generated by a variety of LLMs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Team Members: 毛柏毅, 朱誼學, 許木羽, 張立誠"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers as T\n",
    "from datasets import Dataset\n",
    "import torch\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import kagglehub\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from IPython.display import display, HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HOST: Localhost, IS_RERUN: None\n"
     ]
    }
   ],
   "source": [
    "from typing import Literal\n",
    "\n",
    "HOST: Literal['Localhost', 'Interactive', 'Batch'] = os.environ.get('KAGGLE_KERNEL_RUN_TYPE', 'Localhost')\n",
    "IS_RERUN: bool = os.getenv('KAGGLE_IS_COMPETITION_RERUN')\n",
    "\n",
    "print(f'HOST: {HOST}, IS_RERUN: {IS_RERUN}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\n",
    "    (\"cuda:0\" if torch.cuda.is_available()\n",
    "     else \"mps\" if torch.backends.mps.is_available()\n",
    "     else \"cpu\"))\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_kaggle_csv(dataset: str, name: str, is_comp: bool = False) -> pd.DataFrame:\n",
    "    assert name.endswith('.csv')\n",
    "    if IS_RERUN:\n",
    "        return pd.read_csv(f'/kaggle/input/{dataset}/{name}')\n",
    "    if is_comp:\n",
    "        path = kagglehub.competition_download(dataset)\n",
    "    else:\n",
    "        path = kagglehub.dataset_download(dataset)\n",
    "    return pd.read_csv(Path(path) / name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Looks like you're using an outdated `kagglehub` version (installed: 0.3.5), please consider upgrading to the latest version (0.3.6).\n",
      "(8744, 4)\n"
     ]
    }
   ],
   "source": [
    "if IS_RERUN:\n",
    "    df_train = get_kaggle_csv('daigt-datamix', 'train_essays.csv')\n",
    "    df_test = get_kaggle_csv('llm-detect-ai-generated-text', 'test_essays.csv', is_comp=True)\n",
    "else:\n",
    "    df = get_kaggle_csv('dogeon188/daigt-datamix', 'train_essays.csv')\n",
    "    # split df_train into train and test\n",
    "    df = df.sample(frac=1).reset_index(drop=True)\n",
    "    df_test = df.iloc[-1000:].copy()\n",
    "    df_train = df.iloc[:5000].copy()\n",
    "    # Up sampling -> used to balance the number of data (generated = 0 or 1)\n",
    "    human = df[df['generated']==0].copy().sample(frac=1).reset_index(drop=True)\n",
    "    minority = df_train[df_train['generated']==0].shape[0]\n",
    "    majority = df_train[df_train['generated']==1].shape[0]\n",
    "    up_sampling = human[:majority-minority]\n",
    "    df_train = pd.concat((df_train, up_sampling)).sample(frac=1).reset_index(drop=True)\n",
    "    assert df_train[df_train['generated']==0].shape[0] == df_train[df_train['generated']==1].shape[0]\n",
    "    print(df_train.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T.AutoTokenizer.from_pretrained(\"microsoft/deberta-v3-base\", use_fast=False)\n",
    "tokenizer.save_pretrained(\"./src_for_comp/tokenizer\")\n",
    "\n",
    "# tokenizer = T.AutoTokenizer.from_pretrained(\"src/tokenizer\")\n",
    "\n",
    "# Define the hyperparameters\n",
    "lr = 3e-5\n",
    "epochs = 5\n",
    "train_batch_size = 16\n",
    "validation_batch_size = 16\n",
    "test_batch_size = 16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CustomDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, df, split=\"train\") -> None:\n",
    "        super().__init__()\n",
    "        assert split in [\"train\", \"validation\", \"test\"]\n",
    "        if split != 'test':\n",
    "            self.data = df[split]\n",
    "        else:\n",
    "            self.data = df\n",
    "    def __getitem__(self, index):\n",
    "        d = self.data.iloc[index]\n",
    "        return d\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "def collate_fn(batch):\n",
    "    texts = [item['text'] for item in batch]\n",
    "    generated = [item['generated'] for item in batch]\n",
    "\n",
    "    encoded_inputs = tokenizer(\n",
    "        texts,\n",
    "        return_tensors='pt',\n",
    "        padding=True,\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    generated_tensor = torch.tensor(generated)\n",
    "\n",
    "    return {\n",
    "        'input_ids': encoded_inputs['input_ids'],\n",
    "        'token_type_ids': encoded_inputs['token_type_ids'],\n",
    "        'attention_mask': encoded_inputs['attention_mask'],\n",
    "        'generated': generated_tensor\n",
    "    }\n",
    "\n",
    "split_ratio = 0.85\n",
    "split_idx = int(len(df_train) * split_ratio)\n",
    "df_split = {\"train\": df_train[:split_idx], \"validation\": df_train[split_idx:]}\n",
    "\n",
    "\n",
    "ds_train = CustomDataset(df_split, \"train\")\n",
    "ds_validation = CustomDataset(df_split, \"validation\")\n",
    "dl_train = torch.utils.data.DataLoader(ds_train, batch_size=train_batch_size, collate_fn=collate_fn)\n",
    "dl_validation = torch.utils.data.DataLoader(ds_validation, batch_size=validation_batch_size, collate_fn=collate_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    logits, labels = eval_pred\n",
    "    labels = labels.cpu().numpy()\n",
    "    auc_score = roc_auc_score(labels, logits)\n",
    "\n",
    "    return auc_score\n",
    "\n",
    "class Deberta(torch.nn.Module):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.deberta = T.AutoModel.from_pretrained(\"microsoft/deberta-v3-base\", num_labels=1)\n",
    "        self.deberta.save_pretrained(\"./src_for_comp/deberta\")\n",
    "        self.deberta.gradient_checkpointing_enable()\n",
    "\n",
    "        self.mlp_head = torch.nn.Sequential(\n",
    "            torch.nn.Linear(768, 384),\n",
    "            torch.nn.ReLU(),\n",
    "            # torch.nn.Dropout(p=0.2),\n",
    "            torch.nn.Linear(384, 192),\n",
    "            torch.nn.ReLU(),\n",
    "            torch.nn.Linear(192, 1)\n",
    "        )\n",
    "\n",
    "        self.activation = torch.nn.Sigmoid()\n",
    "    def forward(self, **kwargs):\n",
    "        input_ids = kwargs['input_ids']\n",
    "        attention_mask = kwargs['attention_mask']\n",
    "        outputs = self.deberta(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        last_hidden_state = outputs.last_hidden_state\n",
    "\n",
    "        # Mean pooling\n",
    "        # reshape attention_mask, used to filter the valid tokens\n",
    "        mask = attention_mask.unsqueeze(-1).expand(last_hidden_state.size()).float()  # [batch_size, seq_len, hidden_size]\n",
    "        # summation for valid tokens\n",
    "        sum_embeddings = torch.sum(last_hidden_state * mask, dim=1)  # [batch_size, hidden_size]\n",
    "        # num of valid tokens\n",
    "        sum_mask = torch.clamp(mask.sum(dim=1), min=1e-9)\n",
    "        x = sum_embeddings / sum_mask\n",
    "        \n",
    "        x = self.mlp_head(x)\n",
    "        x = self.activation(x)\n",
    "        \n",
    "        return {\n",
    "            'generated': x\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Deberta().to(device)\n",
    "optimizer = AdamW(model.parameters(), lr=lr, weight_decay=0.01)\n",
    "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "loss_fn = torch.nn.MSELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training epoch [1/5]: 100%|██████████| 465/465 [06:03<00:00,  1.28it/s, Loss=0.1857, ROC AUC=1.0000]\n",
      "Validation epoch [1/5]: 100%|██████████| 82/82 [00:18<00:00,  4.50it/s, ROC AUC=0.9844]\n",
      "Training epoch [2/5]: 100%|██████████| 465/465 [05:59<00:00,  1.29it/s, Loss=0.1626, ROC AUC=1.0000]\n",
      "Validation epoch [2/5]: 100%|██████████| 82/82 [00:18<00:00,  4.49it/s, ROC AUC=0.9844]\n",
      "Training epoch [3/5]: 100%|██████████| 465/465 [06:05<00:00,  1.27it/s, Loss=0.1511, ROC AUC=1.0000]\n",
      "Validation epoch [3/5]: 100%|██████████| 82/82 [00:18<00:00,  4.47it/s, ROC AUC=0.9531]\n",
      "Training epoch [4/5]: 100%|██████████| 465/465 [06:02<00:00,  1.28it/s, Loss=0.0864, ROC AUC=1.0000]\n",
      "Validation epoch [4/5]: 100%|██████████| 82/82 [00:18<00:00,  4.52it/s, ROC AUC=0.9844]\n",
      "Training epoch [5/5]: 100%|██████████| 465/465 [06:01<00:00,  1.29it/s, Loss=0.0066, ROC AUC=1.0000]\n",
      "Validation epoch [5/5]: 100%|██████████| 82/82 [00:18<00:00,  4.52it/s, ROC AUC=0.9844]\n"
     ]
    }
   ],
   "source": [
    "for ep in range(epochs):\n",
    "    pbar = tqdm(dl_train)\n",
    "    pbar.set_description(f\"Training epoch [{ep+1}/{epochs}]\")\n",
    "    model.train()\n",
    "\n",
    "    for batch in pbar:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        generated = batch['generated'].float().to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        pred = model(input_ids = input_ids, attention_mask = attention_mask)\n",
    "        loss = loss_fn(pred['generated'].squeeze(-1), generated)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        pred_gen = list(map(lambda x: x.item(), pred['generated']))\n",
    "        pbar.set_postfix({\"Loss\": f\"{loss:.4f}\", \"ROC AUC\": f\"{compute_metrics((pred_gen, generated)):.4f}\"})\n",
    "        \n",
    "\n",
    "    pbar = tqdm(dl_validation)\n",
    "    pbar.set_description(f\"Validation epoch [{ep+1}/{epochs}]\")\n",
    "    model.eval()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            generated = batch['generated'].to(device)\n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "            # Scoring\n",
    "            pred_gen = list(map(lambda x: x.item(), pred['generated']))\n",
    "            pbar.set_postfix({\"ROC AUC\": f\"{compute_metrics((pred_gen, generated)):.4f}\"})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"./src_for_comp/model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|██████████| 63/63 [00:13<00:00,  4.50it/s]\n"
     ]
    }
   ],
   "source": [
    "model = Deberta().to(device)\n",
    "model.load_state_dict(torch.load(\"./src_for_comp/model\", weights_only=True))\n",
    "\n",
    "ds_test = CustomDataset(df_test.copy(), \"test\")\n",
    "dl_test = torch.utils.data.DataLoader(ds_test, batch_size=test_batch_size, collate_fn=collate_fn)\n",
    "\n",
    "pbar = tqdm(dl_test)\n",
    "pbar.set_description(f\"Test\")\n",
    "model.eval()\n",
    "\n",
    "final_preds = []\n",
    "\n",
    "with torch.no_grad():\n",
    "        for batch in pbar:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            generated = batch['generated'].to(device)\n",
    "            \n",
    "            pred = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "            pred_gen = list(map(lambda x: x.item(), pred['generated']))\n",
    "\n",
    "            final_preds.extend(pred_gen)\n",
    "\n",
    "# final_preds = ...  # should be a 1D array of predictions, with the same length as df_test, and values in [0, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROC AUC: 0.9192\n"
     ]
    }
   ],
   "source": [
    "# validation\n",
    "if not IS_RERUN:\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "\n",
    "    auc_score = roc_auc_score(df_test['generated'], final_preds)\n",
    "    \n",
    "    print(f\"ROC AUC: {auc_score:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['generated'] = final_preds\n",
    "submission = df_test[['id' if IS_RERUN else 'prompt_id', 'generated']]\n",
    "submission.to_csv('submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
